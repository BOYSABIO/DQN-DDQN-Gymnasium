{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c856306b",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Imports y configuraciÃ³n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f33682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import torch\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from tensordict import TensorDict\n",
    "from torch import nn\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c028c",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Wrappers de entorno personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_space = env.observation_space\n",
    "        h, w = obs_space.shape[:2]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(h, w), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(shape, shape), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return cv2.resize(obs, self.shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, num_stack):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = []\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(num_stack, *obs_shape),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.frames = [obs for _ in range(self.num_stack)]\n",
    "        return self._get_observation(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.pop(0)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "env = DQN.SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "state, info = env.reset()\n",
    "action_n = env.action_space.n\n",
    "driver = DQN.Agent(state.shape, action_n, double_q=False)\n",
    "\n",
    "batch_n = 32\n",
    "play_n_episodes = 3000\n",
    "\n",
    "episode_epsilon_list = []\n",
    "episode_reward_list = []\n",
    "episode_length_list = []\n",
    "episode_loss_list = []\n",
    "episode_date_list = []\n",
    "episode_time_list = []\n",
    "\n",
    "episode = 0\n",
    "timestep_n = 0\n",
    "\n",
    "when2learn = 4\n",
    "when2sync = 5000\n",
    "when2save = 100000\n",
    "when2report = 5000\n",
    "when2eval = 50000\n",
    "when2log = 10\n",
    "report_type = 'plot'\n",
    "\n",
    "while episode <= play_n_episodes:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    updating = True\n",
    "    loss_list = []\n",
    "    episode_epsilon_list.append(driver.epsilon)\n",
    "\n",
    "    while updating:\n",
    "        timestep_n += 1\n",
    "        episode_length += 1\n",
    "\n",
    "        action = driver.take_action(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        driver.store(state, action, reward, new_state, terminated)\n",
    "        state = new_state\n",
    "        updating = not (terminated or truncated)\n",
    "\n",
    "        if timestep_n % when2sync == 0:\n",
    "            upd_net_param = driver.updating_net.state_dict()\n",
    "            driver.frozen_net.load_state_dict(upd_net_param)\n",
    "\n",
    "        if timestep_n % when2save == 0:\n",
    "            save_dir = driver.save_dir\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"DQN_{driver.act_taken}.pt\")\n",
    "            torch.save({\n",
    "                'upd_model_state_dict': driver.updating_net.state_dict(),\n",
    "                'frz_model_state_dict': driver.frozen_net.state_dict(),\n",
    "                'optimizer_state_dict': driver.optimizer.state_dict(),\n",
    "                'action_number': driver.act_taken,\n",
    "                'epsilon': driver.epsilon\n",
    "            }, save_path)\n",
    "\n",
    "        if timestep_n % when2learn == 0:\n",
    "            q, loss = driver.update_net(batch_n)\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        if timestep_n % when2report == 0 and report_type == 'text':\n",
    "            print(f'Report: {timestep_n} timestep')\n",
    "            print(f'    episodes: {episode}')\n",
    "            print(f'    n_updates: {driver.n_updates}')\n",
    "            print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "        if timestep_n % when2eval == 0 and report_type == 'text':\n",
    "            rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
    "            eval_reward = torch.clone(rewards_tensor[-50:])\n",
    "            mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "            std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "\n",
    "            lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
    "            eval_length = torch.clone(lengths_tensor[-50:])\n",
    "            mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
    "            std_eval_length = round(torch.std(eval_length).item(), 2)\n",
    "\n",
    "            print(f'Evaluation: {timestep_n} timestep')\n",
    "            print(f'    reward {mean_eval_reward}Â±{std_eval_reward}')\n",
    "            print(f'    episode length {mean_eval_length}Â±{std_eval_length}')\n",
    "            print(f'    episodes: {episode}')\n",
    "            print(f'    n_updates: {driver.n_updates}')\n",
    "            print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    episode_reward_list.append(episode_reward)\n",
    "    episode_length_list.append(episode_length)\n",
    "    episode_loss_list.append(np.mean(loss_list))\n",
    "    now_time = datetime.datetime.now()\n",
    "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
    "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
    "\n",
    "    if report_type == 'plot':\n",
    "        draw_check = DQN.plot_reward(episode, episode_reward_list, timestep_n)\n",
    "\n",
    "    if episode % when2log == 0:\n",
    "        driver.write_log(\n",
    "            episode_date_list,\n",
    "            episode_time_list,\n",
    "            episode_reward_list,\n",
    "            episode_length_list,\n",
    "            episode_loss_list,\n",
    "            episode_epsilon_list,\n",
    "            log_filename='DQN_log_test.csv'\n",
    "        )\n",
    "\n",
    "if report_type == 'text':\n",
    "    rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
    "    eval_reward = torch.clone(rewards_tensor[-100:])\n",
    "    mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "    std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "\n",
    "    lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
    "    eval_length = torch.clone(lengths_tensor[-100:])\n",
    "    mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
    "    std_eval_length = round(torch.std(eval_length).item(), 2)\n",
    "\n",
    "    print(f'Final evaluation: {timestep_n} timestep')\n",
    "    print(f'    reward {mean_eval_reward}Â±{std_eval_reward}')\n",
    "    print(f'    episode length {mean_eval_length}Â±{std_eval_length}')\n",
    "    print(f'    episodes: {episode}')\n",
    "    print(f'    n_updates: {driver.n_updates}')\n",
    "    print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "# Final save\n",
    "save_dir = driver.save_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"DQN_{driver.act_taken}.pt\")\n",
    "torch.save({\n",
    "    'upd_model_state_dict': driver.updating_net.state_dict(),\n",
    "    'frz_model_state_dict': driver.frozen_net.state_dict(),\n",
    "    'optimizer_state_dict': driver.optimizer.state_dict(),\n",
    "    'action_number': driver.act_taken,\n",
    "    'epsilon': driver.epsilon\n",
    "}, save_path)\n",
    "\n",
    "driver.write_log(\n",
    "    episode_date_list,\n",
    "    episode_time_list,\n",
    "    episode_reward_list,\n",
    "    episode_length_list,\n",
    "    episode_loss_list,\n",
    "    episode_epsilon_list,\n",
    "    log_filename='DQN_log_test.csv'\n",
    ")\n",
    "\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68dcd7",
   "metadata": {},
   "source": [
    "## ðŸŽ® ConfiguraciÃ³n del entorno y ejecuciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8578672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0641b9b0",
   "metadata": {},
   "source": [
    "## ðŸ§  ImplementaciÃ³n del modelo DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1dc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import torch\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from tensordict import TensorDict\n",
    "from torch import nn\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        cannel_n, height, width = in_dim\n",
    "\n",
    "        if height != 84 or width != 84:\n",
    "            raise ValueError(f\"DQN model requires input of a (84, 84)-shape. Input of a ({height, width})-shape was passed.\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=cannel_n, out_channels=16,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                      kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2592, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_space_shape,\n",
    "            action_n,\n",
    "            load_state=False,\n",
    "            load_model=None,\n",
    "            double_q=False,\n",
    "            gamma = 0.95,\n",
    "            epsilon = 1,\n",
    "            epsilon_decay = 0.9999925,\n",
    "            epsilon_min = 0.05\n",
    "            ):\n",
    "        \n",
    "        self.gamma = gamma # discounting factor\n",
    "        self.epsilon = epsilon # exploration rate\n",
    "        self.epsilon_decay = epsilon_decay # decay of exploration rate\n",
    "        self.epsilon_min = epsilon_min # minimum value of exploration rate\n",
    "        self.state_shape = state_space_shape\n",
    "        self.action_n = action_n\n",
    "        self.load_state = load_state # purpose of loading the model (training or evaluation)\n",
    "        self.double_q = double_q # if True, Double DQN is used\n",
    "        self.save_dir = './training/saved_models/'\n",
    "        self.log_dir = './training/logs/'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.updating_net = DQN(self.state_shape, self.action_n).float()\n",
    "        self.updating_net = self.updating_net.to(device=self.device)\n",
    "        self.frozen_net = DQN(self.state_shape, self.action_n).float()\n",
    "        self.frozen_net = self.frozen_net.to(device=self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.updating_net.parameters(),\n",
    "                                          lr=0.0002)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        self.buffer = TensorDictReplayBuffer(\n",
    "                storage=LazyMemmapStorage(\n",
    "                    300000,\n",
    "                    device=torch.device(\"cpu\")))\n",
    "        self.act_taken = 0\n",
    "        self.n_updates = 0\n",
    "        if load_state:\n",
    "            if load_model == None:\n",
    "                raise ValueError(f\"Specify a model name for loading.\")\n",
    "            load_dir = self.save_dir\n",
    "            self.load_model = load_model\n",
    "            self.load(load_dir, load_model)\n",
    "        \n",
    "\n",
    "    def store(self, state, action, reward, new_state, terminated):\n",
    "        self.buffer.add(TensorDict({\n",
    "                    \"state\": torch.tensor(state),\n",
    "                    \"action\": torch.tensor(action),\n",
    "                    \"reward\": torch.tensor(reward),\n",
    "                    \"new_state\": torch.tensor(new_state),\n",
    "                    \"terminated\": torch.tensor(terminated)\n",
    "                    }, batch_size=[]))\n",
    "\n",
    "    def get_samples(self, batch_size):\n",
    "        batch = self.buffer.sample(\n",
    "            batch_size)\n",
    "        states = batch.get('state').type(torch.FloatTensor).to(self.device)\n",
    "        new_states = batch.get('new_state').type(torch.FloatTensor).to(self.device)\n",
    "        actions = batch.get('action').squeeze().to(self.device)\n",
    "        rewards = batch.get('reward').squeeze().to(self.device)\n",
    "        terminateds = batch.get('terminated').squeeze().to(self.device)\n",
    "        return states, actions, rewards, new_states, terminateds\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_idx = np.random.randint(self.action_n)\n",
    "        else:\n",
    "            state = torch.tensor(\n",
    "                state,\n",
    "                dtype=torch.float32,\n",
    "                device=self.device\n",
    "                ).unsqueeze(0)\n",
    "            action_values = self.updating_net(state)\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        self.act_taken += 1\n",
    "        return action_idx\n",
    "\n",
    "    def update_net(self, batch_size):\n",
    "        self.n_updates += 1\n",
    "        states, actions, rewards, \\\n",
    "            new_states, terminateds = self.get_samples(batch_size)\n",
    "        action_values = self.updating_net(states)\n",
    "        td_est = action_values[np.arange(batch_size), actions]\n",
    "        if self.double_q:\n",
    "            with torch.no_grad():\n",
    "                next_actions = torch.argmax(self.updating_net(new_states), axis=1)\n",
    "                tar_action_values = self.frozen_net(new_states)\n",
    "            td_tar = rewards + (1 - terminateds.float()) \\\n",
    "                * self.gamma*tar_action_values[np.arange(batch_size), next_actions]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                tar_action_values = self.frozen_net(new_states)\n",
    "            td_tar = rewards + (1 - terminateds.float()) * self.gamma*tar_action_values.max(1)[0]\n",
    "        loss = self.loss_fn(td_est, td_tar)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        return td_est, loss\n",
    "\n",
    "    def save(self, save_dir, save_name):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = save_dir + save_name + f\"_{self.act_taken}.pt\"\n",
    "        torch.save({\n",
    "            'upd_model_state_dict': self.updating_net.state_dict(),\n",
    "            'frz_model_state_dict': self.frozen_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'replay_buffer': self.buffer,\n",
    "            'action_number': self.act_taken,\n",
    "            'epsilon': self.epsilon\n",
    "            }, save_path)\n",
    "        print(f\"Model saved to {save_path} at step {self.act_taken}\")\n",
    "\n",
    "    def load(self, load_dir, model_name):\n",
    "        loaded_model = torch.load(load_dir+model_name)\n",
    "        upd_net_param = loaded_model['upd_model_state_dict']\n",
    "        frz_net_param = loaded_model['frz_model_state_dict']\n",
    "        opt_param = loaded_model['optimizer_state_dict']\n",
    "        self.updating_net.load_state_dict(upd_net_param)\n",
    "        self.frozen_net.load_state_dict(frz_net_param)\n",
    "        self.optimizer.load_state_dict(opt_param)\n",
    "        if self.load_state == 'eval':\n",
    "            self.updating_net.eval()\n",
    "            self.frozen_net.eval()\n",
    "            self.epsilon_min = 0\n",
    "            self.epsilon = 0\n",
    "        elif self.load_state == 'train':\n",
    "            self.updating_net.train()\n",
    "            self.frozen_net.train()\n",
    "            self.act_taken = loaded_model['action_number']\n",
    "            self.epsilon = loaded_model['epsilon']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown load state. Should be either 'eval' or 'train'.\")\n",
    "        \n",
    "    def write_log(\n",
    "            self,\n",
    "            date_list,\n",
    "            time_list,\n",
    "            reward_list,\n",
    "            length_list,\n",
    "            loss_list,\n",
    "            epsilon_list,\n",
    "            log_filename='default_log.csv'\n",
    "            ):\n",
    "\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        rows = [['date']+date_list,\n",
    "                ['time']+time_list,\n",
    "                ['reward']+reward_list,\n",
    "                ['length']+length_list,\n",
    "                ['loss']+loss_list,\n",
    "                ['epsilon']+epsilon_list]\n",
    "        with open(self.log_dir+log_filename, 'w') as csvfile:  \n",
    "            csvwriter = csv.writer(csvfile)    \n",
    "            csvwriter.writerows(rows)\n",
    "\n",
    "\n",
    "def plot_reward(episode_num, reward_list, n_steps):\n",
    "    plt.figure(1)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "    if len(rewards_tensor) >= 11:\n",
    "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "        plt.clf()\n",
    "        plt.title(f'Episode #{episode_num}: {n_steps} steps, \\\n",
    "                  reward {mean_eval_reward}Â±{std_eval_reward}')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_tensor.numpy())\n",
    "    if len(rewards_tensor) >= 50:\n",
    "        reward_f = torch.clone(rewards_tensor[:50])\n",
    "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.ones(49)*torch.mean(reward_f), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
