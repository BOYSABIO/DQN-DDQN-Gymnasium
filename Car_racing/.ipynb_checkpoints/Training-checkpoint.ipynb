{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x156067320>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================\n",
    "# 1) IMPORTING LIBRARIES\n",
    "# ====================================\n",
    "import os\n",
    "import matplotlib\n",
    "import torch\n",
    "import datetime\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from tensordict import TensorDict\n",
    "from torch import nn\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 2) GYMNASIUM WRAPPER FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def create_skip_frame_wrapper(env, skip=4):\n",
    "    \"\"\"Skip frames wrapper to reduce computational load\"\"\"\n",
    "    class SkipFrame(gym.Wrapper):\n",
    "        def __init__(self, env, skip):\n",
    "            super().__init__(env)\n",
    "            self._skip = skip\n",
    "\n",
    "        def step(self, action):\n",
    "            total_reward = 0.0\n",
    "            for _ in range(self._skip):\n",
    "                state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if terminated:\n",
    "                    break\n",
    "            return state, total_reward, terminated, truncated, info\n",
    "    \n",
    "    return SkipFrame(env, skip)\n",
    "\n",
    "def create_grayscale_wrapper(env):\n",
    "    \"\"\"Convert RGB observations to grayscale\"\"\"\n",
    "    class GrayScaleObservation(gym.ObservationWrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            obs_space = env.observation_space\n",
    "            h, w = obs_space.shape[:2]\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255, shape=(h, w), dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        def observation(self, obs):\n",
    "            return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    return GrayScaleObservation(env)\n",
    "\n",
    "def create_resize_wrapper(env, shape=84):\n",
    "    \"\"\"Resize observations to specified shape\"\"\"\n",
    "    class ResizeObservation(gym.ObservationWrapper):\n",
    "        def __init__(self, env, shape):\n",
    "            super().__init__(env)\n",
    "            self.shape = (shape, shape)\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255, shape=(shape, shape), dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        def observation(self, obs):\n",
    "            return cv2.resize(obs, self.shape, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return ResizeObservation(env, shape)\n",
    "\n",
    "def create_frame_stack_wrapper(env, num_stack=4):\n",
    "    \"\"\"Stack multiple frames to provide temporal information\"\"\"\n",
    "    class FrameStack(gym.Wrapper):\n",
    "        def __init__(self, env, num_stack):\n",
    "            super().__init__(env)\n",
    "            self.num_stack = num_stack\n",
    "            self.frames = []\n",
    "            obs_shape = env.observation_space.shape\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0,\n",
    "                high=255,\n",
    "                shape=(num_stack, *obs_shape),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "            self.frames = [obs for _ in range(self.num_stack)]\n",
    "            return self._get_observation(), info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            self.frames.pop(0)\n",
    "            self.frames.append(obs)\n",
    "            return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "        def _get_observation(self):\n",
    "            return np.stack(self.frames, axis=0)\n",
    "    \n",
    "    return FrameStack(env, num_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 3) DQN NEURAL NETWORK FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def create_dqn_network(in_dim, out_dim):\n",
    "    \"\"\"Create DQN neural network\"\"\"\n",
    "    channel_n, height, width = in_dim\n",
    "    \n",
    "    if height != 84 or width != 84:\n",
    "        raise ValueError(f\"DQN model requires input of a (84, 84)-shape. Input of a ({height, width})-shape was passed.\")\n",
    "    \n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=channel_n, out_channels=16,\n",
    "                  kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                  kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(2592, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_dim),\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 4) AGENT STATE AND BUFFER FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def initialize_agent(state_space_shape, action_n, gamma=0.95, epsilon=1, \n",
    "                    epsilon_decay=0.9999925, epsilon_min=0.05, double_q=False):\n",
    "    \"\"\"Initialize agent parameters and networks\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    agent_state = {\n",
    "        'gamma': gamma,\n",
    "        'epsilon': epsilon,\n",
    "        'epsilon_decay': epsilon_decay,\n",
    "        'epsilon_min': epsilon_min,\n",
    "        'state_shape': state_space_shape,\n",
    "        'action_n': action_n,\n",
    "        'double_q': double_q,\n",
    "        'save_dir': './training/saved_models/',\n",
    "        'log_dir': './training/logs/',\n",
    "        'device': device,\n",
    "        'act_taken': 0,\n",
    "        'n_updates': 0\n",
    "    }\n",
    "    \n",
    "    # Create networks\n",
    "    updating_net = create_dqn_network(state_space_shape, action_n).float().to(device)\n",
    "    frozen_net = create_dqn_network(state_space_shape, action_n).float().to(device)\n",
    "    \n",
    "    # Create optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(updating_net.parameters(), lr=0.0002)\n",
    "    loss_fn = torch.nn.SmoothL1Loss()\n",
    "    \n",
    "    # Create replay buffer\n",
    "    buffer = TensorDictReplayBuffer(\n",
    "        storage=LazyMemmapStorage(300000, device=torch.device(\"cpu\"))\n",
    "    )\n",
    "    \n",
    "    return agent_state, updating_net, frozen_net, optimizer, loss_fn, buffer\n",
    "\n",
    "def store_experience(buffer, state, action, reward, new_state, terminated):\n",
    "    \"\"\"Store experience in replay buffer\"\"\"\n",
    "    buffer.add(TensorDict({\n",
    "        \"state\": torch.tensor(state),\n",
    "        \"action\": torch.tensor(action),\n",
    "        \"reward\": torch.tensor(reward),\n",
    "        \"new_state\": torch.tensor(new_state),\n",
    "        \"terminated\": torch.tensor(terminated)\n",
    "    }, batch_size=[]))\n",
    "\n",
    "def get_batch_samples(buffer, batch_size, device):\n",
    "    \"\"\"Sample batch from replay buffer\"\"\"\n",
    "    batch = buffer.sample(batch_size)\n",
    "    states = batch.get('state').type(torch.FloatTensor).to(device)\n",
    "    new_states = batch.get('new_state').type(torch.FloatTensor).to(device)\n",
    "    actions = batch.get('action').squeeze().to(device)\n",
    "    rewards = batch.get('reward').squeeze().to(device)\n",
    "    terminateds = batch.get('terminated').squeeze().to(device)\n",
    "    return states, actions, rewards, new_states, terminateds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 5) ACTION SELECTION AND LEARNING FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def take_action(agent_state, updating_net, state):\n",
    "    \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.rand() < agent_state['epsilon']:\n",
    "        action_idx = np.random.randint(agent_state['action_n'])\n",
    "    else:\n",
    "        state_tensor = torch.tensor(\n",
    "            state, dtype=torch.float32, device=agent_state['device']\n",
    "        ).unsqueeze(0)\n",
    "        action_values = updating_net(state_tensor)\n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "    \n",
    "    # Update epsilon\n",
    "    if agent_state['epsilon'] > agent_state['epsilon_min']:\n",
    "        agent_state['epsilon'] *= agent_state['epsilon_decay']\n",
    "    else:\n",
    "        agent_state['epsilon'] = agent_state['epsilon_min']\n",
    "    \n",
    "    agent_state['act_taken'] += 1\n",
    "    return action_idx\n",
    "\n",
    "def update_network(agent_state, updating_net, frozen_net, optimizer, loss_fn, buffer, batch_size):\n",
    "    \"\"\"Update the neural network using experience replay\"\"\"\n",
    "    agent_state['n_updates'] += 1\n",
    "    \n",
    "    states, actions, rewards, new_states, terminateds = get_batch_samples(\n",
    "        buffer, batch_size, agent_state['device']\n",
    "    )\n",
    "    \n",
    "    action_values = updating_net(states)\n",
    "    td_est = action_values[np.arange(batch_size), actions]\n",
    "    \n",
    "    if agent_state['double_q']:\n",
    "        with torch.no_grad():\n",
    "            next_actions = torch.argmax(updating_net(new_states), axis=1)\n",
    "            tar_action_values = frozen_net(new_states)\n",
    "        td_tar = rewards + (1 - terminateds.float()) * agent_state['gamma'] * \\\n",
    "                tar_action_values[np.arange(batch_size), next_actions]\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            tar_action_values = frozen_net(new_states)\n",
    "        td_tar = rewards + (1 - terminateds.float()) * agent_state['gamma'] * \\\n",
    "                tar_action_values.max(1)[0]\n",
    "    \n",
    "    loss = loss_fn(td_est, td_tar)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return td_est, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 6) SAVE/LOAD AND LOGGING FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def save_model(agent_state, updating_net, frozen_net, optimizer, save_name=None):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    save_dir = agent_state['save_dir']\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    if save_name is None:\n",
    "        save_name = f\"DQN_{agent_state['act_taken']}\"\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f\"{save_name}.pt\")\n",
    "    torch.save({\n",
    "        'upd_model_state_dict': updating_net.state_dict(),\n",
    "        'frz_model_state_dict': frozen_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'action_number': agent_state['act_taken'],\n",
    "        'epsilon': agent_state['epsilon']\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to {save_path} at step {agent_state['act_taken']}\")\n",
    "\n",
    "def write_log(agent_state, date_list, time_list, reward_list, length_list, \n",
    "              loss_list, epsilon_list, log_filename='training_log.csv'):\n",
    "    \"\"\"Write training log to CSV file\"\"\"\n",
    "    log_dir = agent_state['log_dir']\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    rows = [['date'] + date_list,\n",
    "            ['time'] + time_list,\n",
    "            ['reward'] + reward_list,\n",
    "            ['length'] + length_list,\n",
    "            ['loss'] + loss_list,\n",
    "            ['epsilon'] + epsilon_list]\n",
    "    \n",
    "    with open(os.path.join(log_dir, log_filename), 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 7) PLOTTING AND VISUALIZATION FUNCTIONS\n",
    "# ====================================\n",
    "\n",
    "def plot_reward(episode_num, reward_list, n_steps):\n",
    "    \"\"\"Plot training rewards with moving average\"\"\"\n",
    "    plt.figure(1)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "    \n",
    "    if len(rewards_tensor) >= 11:\n",
    "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "        plt.clf()\n",
    "        plt.title(f'Episode #{episode_num}: {n_steps} steps, '\n",
    "                 f'reward {mean_eval_reward}±{std_eval_reward}')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_tensor.numpy())\n",
    "    \n",
    "    if len(rewards_tensor) >= 50:\n",
    "        reward_f = torch.clone(rewards_tensor[:50])\n",
    "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.ones(49) * torch.mean(reward_f), means))\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "def print_evaluation(timestep_n, episode, n_updates, epsilon, reward_list, length_list, eval_window=50):\n",
    "    \"\"\"Print evaluation statistics\"\"\"\n",
    "    if len(reward_list) >= eval_window:\n",
    "        rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "        eval_reward = torch.clone(rewards_tensor[-eval_window:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "\n",
    "        lengths_tensor = torch.tensor(length_list, dtype=torch.float)\n",
    "        eval_length = torch.clone(lengths_tensor[-eval_window:])\n",
    "        mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
    "        std_eval_length = round(torch.std(eval_length).item(), 2)\n",
    "\n",
    "        print(f'Evaluation: {timestep_n} timestep')\n",
    "        print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
    "        print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
    "        print(f'    episodes: {episode}')\n",
    "        print(f'    n_updates: {n_updates}')\n",
    "        print(f'    epsilon: {epsilon}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 8) ENVIRONMENT SETUP FUNCTION\n",
    "# ====================================\n",
    "\n",
    "def setup_environment(env_name=\"CarRacing-v3\", skip_frames=4, resize_shape=84, frame_stack=4):\n",
    "    \"\"\"Setup and wrap the environment\"\"\"\n",
    "    env = gym.make(env_name, continuous=False)\n",
    "    env = create_skip_frame_wrapper(env, skip=skip_frames)\n",
    "    env = create_grayscale_wrapper(env)\n",
    "    env = create_resize_wrapper(env, shape=resize_shape)\n",
    "    env = create_frame_stack_wrapper(env, num_stack=frame_stack)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 9) MAIN TRAINING FUNCTION\n",
    "# ====================================\n",
    "\n",
    "def train_dqn(play_n_episodes=3000, batch_size=32, double_q=False, \n",
    "              when2learn=4, when2sync=5000, when2save=100000, \n",
    "              when2report=5000, when2eval=50000, when2log=10,\n",
    "              report_type='plot'):\n",
    "    \"\"\"Main DQN training function\"\"\"\n",
    "    \n",
    "    # Setup environment\n",
    "    env = setup_environment()\n",
    "    state, info = env.reset()\n",
    "    action_n = env.action_space.n\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent_state, updating_net, frozen_net, optimizer, loss_fn, buffer = initialize_agent(\n",
    "        state.shape, action_n, double_q=double_q\n",
    "    )\n",
    "    \n",
    "    # Training tracking variables\n",
    "    episode_epsilon_list = []\n",
    "    episode_reward_list = []\n",
    "    episode_length_list = []\n",
    "    episode_loss_list = []\n",
    "    episode_date_list = []\n",
    "    episode_time_list = []\n",
    "    \n",
    "    episode = 0\n",
    "    timestep_n = 0\n",
    "    \n",
    "    # Training loop\n",
    "    while episode <= play_n_episodes:\n",
    "        episode += 1\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        updating = True\n",
    "        loss_list = []\n",
    "        episode_epsilon_list.append(agent_state['epsilon'])\n",
    "        \n",
    "        # Episode loop\n",
    "        while updating:\n",
    "            timestep_n += 1\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Take action and step environment\n",
    "            action = take_action(agent_state, updating_net, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store experience\n",
    "            store_experience(buffer, state, action, reward, new_state, terminated)\n",
    "            state = new_state\n",
    "            updating = not (terminated or truncated)\n",
    "            \n",
    "            # Sync target network\n",
    "            if timestep_n % when2sync == 0:\n",
    "                upd_net_param = updating_net.state_dict()\n",
    "                frozen_net.load_state_dict(upd_net_param)\n",
    "            \n",
    "            # Save model\n",
    "            if timestep_n % when2save == 0:\n",
    "                save_model(agent_state, updating_net, frozen_net, optimizer)\n",
    "            \n",
    "            # Update network\n",
    "            if timestep_n % when2learn == 0 and len(buffer) > batch_size:\n",
    "                q, loss = update_network(agent_state, updating_net, frozen_net, \n",
    "                                       optimizer, loss_fn, buffer, batch_size)\n",
    "                loss_list.append(loss)\n",
    "            \n",
    "            # Report progress\n",
    "            if timestep_n % when2report == 0 and report_type == 'text':\n",
    "                print(f'Report: {timestep_n} timestep')\n",
    "                print(f'    episodes: {episode}')\n",
    "                print(f'    n_updates: {agent_state[\"n_updates\"]}')\n",
    "                print(f'    epsilon: {agent_state[\"epsilon\"]}')\n",
    "            \n",
    "            # Evaluation\n",
    "            if timestep_n % when2eval == 0 and report_type == 'text':\n",
    "                print_evaluation(timestep_n, episode, agent_state['n_updates'], \n",
    "                               agent_state['epsilon'], episode_reward_list, episode_length_list)\n",
    "        \n",
    "        # Reset environment for next episode\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        # Record episode statistics\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        episode_length_list.append(episode_length)\n",
    "        episode_loss_list.append(np.mean(loss_list) if loss_list else 0)\n",
    "        now_time = datetime.datetime.now()\n",
    "        episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
    "        episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
    "        \n",
    "        # Plot progress\n",
    "        if report_type == 'plot':\n",
    "            plot_reward(episode, episode_reward_list, timestep_n)\n",
    "        \n",
    "        # Write logs\n",
    "        if episode % when2log == 0:\n",
    "            write_log(agent_state, episode_date_list, episode_time_list,\n",
    "                     episode_reward_list, episode_length_list, episode_loss_list,\n",
    "                     episode_epsilon_list, log_filename='DQN_training_log.csv')\n",
    "    \n",
    "    # Final evaluation and save\n",
    "    if report_type == 'text':\n",
    "        print_evaluation(timestep_n, episode, agent_state['n_updates'], \n",
    "                        agent_state['epsilon'], episode_reward_list, episode_length_list, 100)\n",
    "    \n",
    "    # Final save\n",
    "    save_model(agent_state, updating_net, frozen_net, optimizer, \"DQN_final\")\n",
    "    write_log(agent_state, episode_date_list, episode_time_list,\n",
    "             episode_reward_list, episode_length_list, episode_loss_list,\n",
    "             episode_epsilon_list, log_filename='DQN_final_log.csv')\n",
    "    \n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    return episode_reward_list, episode_length_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 10) MAIN EXECUTION\n",
    "# ====================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run training\n",
    "    reward_history, length_history = train_dqn(\n",
    "        play_n_episodes=3000,\n",
    "        batch_size=32,\n",
    "        double_q=False,     # Change to 'True' for DDQN\n",
    "        report_type='plot'  # Change to 'text' for text-based reporting\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
