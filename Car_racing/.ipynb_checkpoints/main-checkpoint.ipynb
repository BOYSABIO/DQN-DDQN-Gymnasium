{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sz4KmoLdM3lT",
    "outputId": "f713ec8c-9c82-452a-bd83-32299e6f2d44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/tensordict/_pytree.py:180: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/tensordict/_pytree.py:199: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchrl/data/replay_buffers/samplers.py:34: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/tensordict/_pytree.py:180: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x15a0774d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import torch\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import DQN_model as DQN\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from tensordict import TensorDict\n",
    "from torch import nn\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_space = env.observation_space\n",
    "        h, w = obs_space.shape[:2]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(h, w), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(shape, shape), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return cv2.resize(obs, self.shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, num_stack):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = []\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(num_stack, *obs_shape),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.frames = [obs for _ in range(self.num_stack)]\n",
    "        return self._get_observation(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.pop(0)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.stack(self.frames, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "env = DQN.SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "state, info = env.reset()\n",
    "action_n = env.action_space.n\n",
    "driver = DQN.Agent(state.shape, action_n, double_q=False)\n",
    "\n",
    "batch_n = 32\n",
    "play_n_episodes = 3000\n",
    "\n",
    "episode_epsilon_list = []\n",
    "episode_reward_list = []\n",
    "episode_length_list = []\n",
    "episode_loss_list = []\n",
    "episode_date_list = []\n",
    "episode_time_list = []\n",
    "\n",
    "episode = 0\n",
    "timestep_n = 0\n",
    "\n",
    "when2learn = 4\n",
    "when2sync = 5000\n",
    "when2save = 100000\n",
    "when2report = 5000\n",
    "when2eval = 50000\n",
    "when2log = 10\n",
    "report_type = 'plot'\n",
    "\n",
    "while episode <= play_n_episodes:\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    updating = True\n",
    "    loss_list = []\n",
    "    episode_epsilon_list.append(driver.epsilon)\n",
    "\n",
    "    while updating:\n",
    "        timestep_n += 1\n",
    "        episode_length += 1\n",
    "\n",
    "        action = driver.take_action(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        driver.store(state, action, reward, new_state, terminated)\n",
    "        state = new_state\n",
    "        updating = not (terminated or truncated)\n",
    "\n",
    "        if timestep_n % when2sync == 0:\n",
    "            upd_net_param = driver.updating_net.state_dict()\n",
    "            driver.frozen_net.load_state_dict(upd_net_param)\n",
    "\n",
    "        if timestep_n % when2save == 0:\n",
    "            save_dir = driver.save_dir\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"DQN_{driver.act_taken}.pt\")\n",
    "            torch.save({\n",
    "                'upd_model_state_dict': driver.updating_net.state_dict(),\n",
    "                'frz_model_state_dict': driver.frozen_net.state_dict(),\n",
    "                'optimizer_state_dict': driver.optimizer.state_dict(),\n",
    "                'action_number': driver.act_taken,\n",
    "                'epsilon': driver.epsilon\n",
    "            }, save_path)\n",
    "\n",
    "        if timestep_n % when2learn == 0:\n",
    "            q, loss = driver.update_net(batch_n)\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        if timestep_n % when2report == 0 and report_type == 'text':\n",
    "            print(f'Report: {timestep_n} timestep')\n",
    "            print(f'    episodes: {episode}')\n",
    "            print(f'    n_updates: {driver.n_updates}')\n",
    "            print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "        if timestep_n % when2eval == 0 and report_type == 'text':\n",
    "            rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
    "            eval_reward = torch.clone(rewards_tensor[-50:])\n",
    "            mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "            std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "\n",
    "            lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
    "            eval_length = torch.clone(lengths_tensor[-50:])\n",
    "            mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
    "            std_eval_length = round(torch.std(eval_length).item(), 2)\n",
    "\n",
    "            print(f'Evaluation: {timestep_n} timestep')\n",
    "            print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
    "            print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
    "            print(f'    episodes: {episode}')\n",
    "            print(f'    n_updates: {driver.n_updates}')\n",
    "            print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    episode_reward_list.append(episode_reward)\n",
    "    episode_length_list.append(episode_length)\n",
    "    episode_loss_list.append(np.mean(loss_list))\n",
    "    now_time = datetime.datetime.now()\n",
    "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
    "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
    "\n",
    "    if report_type == 'plot':\n",
    "        draw_check = DQN.plot_reward(episode, episode_reward_list, timestep_n)\n",
    "\n",
    "    if episode % when2log == 0:\n",
    "        driver.write_log(\n",
    "            episode_date_list,\n",
    "            episode_time_list,\n",
    "            episode_reward_list,\n",
    "            episode_length_list,\n",
    "            episode_loss_list,\n",
    "            episode_epsilon_list,\n",
    "            log_filename='DQN_log_test.csv'\n",
    "        )\n",
    "\n",
    "if report_type == 'text':\n",
    "    rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
    "    eval_reward = torch.clone(rewards_tensor[-100:])\n",
    "    mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "    std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "\n",
    "    lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
    "    eval_length = torch.clone(lengths_tensor[-100:])\n",
    "    mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
    "    std_eval_length = round(torch.std(eval_length).item(), 2)\n",
    "\n",
    "    print(f'Final evaluation: {timestep_n} timestep')\n",
    "    print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
    "    print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
    "    print(f'    episodes: {episode}')\n",
    "    print(f'    n_updates: {driver.n_updates}')\n",
    "    print(f'    epsilon: {driver.epsilon}')\n",
    "\n",
    "# Final save\n",
    "save_dir = driver.save_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"DQN_{driver.act_taken}.pt\")\n",
    "torch.save({\n",
    "    'upd_model_state_dict': driver.updating_net.state_dict(),\n",
    "    'frz_model_state_dict': driver.frozen_net.state_dict(),\n",
    "    'optimizer_state_dict': driver.optimizer.state_dict(),\n",
    "    'action_number': driver.act_taken,\n",
    "    'epsilon': driver.epsilon\n",
    "}, save_path)\n",
    "\n",
    "driver.write_log(\n",
    "    episode_date_list,\n",
    "    episode_time_list,\n",
    "    episode_reward_list,\n",
    "    episode_length_list,\n",
    "    episode_loss_list,\n",
    "    episode_epsilon_list,\n",
    "    log_filename='DQN_log_test.csv'\n",
    ")\n",
    "\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
